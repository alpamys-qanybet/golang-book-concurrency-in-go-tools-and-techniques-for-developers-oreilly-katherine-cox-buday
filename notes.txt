CHAPTER 1 An Introduction to Concurrency

If software was web scale, among other things, you could expect that it would be embarrassingly parallel; that is, web scale software is usually expected to be able to handle hundreds of thousands (or more) of simultaneous workloads by adding more instances of the application.

A race condition occurs when two or more operations must execute in the correct order, but the program has not been written so that this order is guaranteed to be maintained.
Most of the time, this shows up in what’s called a data race, where one concurrent operation attempts to read a variable while at some undetermined time another con‐ current operation is attempting to write to the same variable.

Atomicity
When something is considered atomic, or to have the property of atomicity, this means that within the context that it is operating, it is indivisible, or uninterruptible.


Deadlock
A deadlocked program is one in which all concurrent processes are waiting on one another. In this state, the program will never recover without outside intervention.

The conditions are now known as the Coffman Conditions and are the basis for techniques that help detect, prevent, and correct deadlocks.
The Coffman Conditions are as follows:

Mutual Exclusion
A concurrent process holds exclusive rights to a resource at any one time.

Wait For Condition
A concurrent process must simultaneously hold a resource and be waiting for an additional resource.

No Preemption
A resource held by a concurrent process can only be released by that process, so it fulfills this condition.

Circular Wait
A concurrent process (P1) must be waiting on a chain of other concurrent pro‐ cesses (P2), which are in turn waiting on it (P1), so it fulfills this final condition too.



Livelock
Livelocks are programs that are actively performing concurrent operations, but these operations do nothing to move the state of the program forward.


Starvation
Starvation is any situation where a concurrent process cannot get all the resources it needs to perform work.


 As of Go 1.8, garbage collection pauses are generally between 10 and 100 microseconds!


=========================================================================================================================


CHAPTER 2 Modeling Your Code: Communicating Sequential Processes


Concurrency is a property of the code; parallelism is a property of the running program.


Before Go if you wanted to write concurrent code, you would model your program in terms of threads and synchronize the access to the memory between them. If you had a lot of things you had to model concurrently and your machine couldn’t handle that many threads, you created a thread pool and multiplexed your operations onto the thread pool.

Go has added another link in that chain: the goroutine. In addition, Go has borrowed several concepts from the work of famed computer scientist Tony Hoare, and introduced new primitives for us to use, namely channels.

We’d assume that introducing another level of abstraction below OS threads would bring with it more difficulties, but the interesting thing is that it doesn’t. It actually makes things easier. This is because we haven’t really added another layer of abstraction on top of OS threads, we’ve supplanted them.

Threads are still there, of course, but we find that we rarely have to think about our problem space in terms of OS threads. Instead, we model things in goroutines and channels, and occasionally shared memory.

CSP stands for “Communicating Sequential Processes,” which is both a technique and the name of the paper that introduced it. In 1978, Charles Antony Richard Hoare published the paper in the Association for Computing Machinery (ACM).

Inputs and outputs needed to be considered language primitives, Hoare’s CSP programming language contained primitives to model input and output, or communication, between processes correctly (this is where the paper’s name comes from). Hoare applied the term processes to any encapsulated portion of logic that required input to run and produced output other processes would consume.

For communication between the processes, Hoare created input and output com‐ mands: ! for sending input into a process, and ? for reading output from a process.

Each command had to specify either an output variable (in the case of reading a variable out of a process), or a destination (in the case of sending input to a process). Sometimes these two would refer to the same thing, in which case the two processes would be said to correspond. In other words, output from one process would flow directly into the input of another process.

Operation 							Explanation

cardreader?cardimage				From cardreader, read a card and assign its value (an array of characters) to the 
									variable cardimage.

lineprinter!line					To lineprinter, send the value of lineimage for printing.

image X?(x, y)						From process named X, input a pair of values and assign them to x and y.

DIV!(3*a+b, 13)						To process DIV, output the two specified values.

*[c:character; west?c → east!c]		Read all the characters output by west, and output them one by one to east. 
									The repetition  terminates when the process west terminates.

The similarities to Go’s channels are apparent. Notice how in the last example the output from west was sent to a variable c and the input to east was received from the same variable. These two processes correspond.

Over the next six years, the idea of CSP was refined into a formal representation of something called process calculus in an effort to take the ideas of communicating sequential processes and actually begin to reason about program correctness. Process calculus is a way to mathematically model concurrent systems and also provides algebraic laws to perform transformations on these systems to analyze their various properties, e.g., efficiency and correctness.

The language also utilized a so-called guarded command, which Edgar Dijkstra had introduced in 1974. (“Guarded commands, nondetermi‐ nacy and formal derivation of programs”).

A guarded command is simply a statement with a left- and righthand side, split by a →.

The lefthand side served as a conditional, or guard for the righthand side in that if the lefthand side was false or, in the case of a command, returned false or had exited, the righthand side would never be executed.

Combining these with Hoare’s I/O commands laid the foundation for Hoare’s communicating processes, and thus Go’s channels.

It’s common for languages to end their chain of abstraction at the level of the OS thread and memory access synchronization. Go takes a different route and supplants this with the concept of goroutines and channels.



Goroutines are lightweight, and we normally won’t have to worry about creating one.

Go’s runtime multiplexes goroutines onto OS threads automatically and manages their scheduling for us.

Channels, for instance, are inherently composable with other channels. This makes writing large systems simpler because you can coordinate the input from multiple subsystems by easily composing the output together. You can combine input channels with timeouts, cancellations, or messages to other subsystems. Coordinating mutexes is a much more difficult proposition.

The select statement is the complement to Go’s channels and is what enables all the difficult bits of composing channels. select statements allow you to efficiently wait for events, select a message from competing channels in a uniform random way, continue on if there are no messages waiting, and more.


Go was designed around CSP; however, Go also supports more traditional means of writing concurrent code through memory access synchronization and the primitives that follow that technique. Structs and methods in the sync and other packages allow you to perform locks, create pools of resources, preempt goroutines, and more.

Package sync provides basic synchronization primitives such as mutual exclusion locks. Other than the Once and WaitGroup types, most are intended for use by low-level library routines. Higher-level synchronization is better done via channels and communication.


Consider structuring your program so that only one goroutine at a time is ever responsible for a particular piece of data. Do not communicate by sharing memory. Instead, share memory by communicating.



Are you trying to transfer ownership of data? Use channels
If you have a bit of code that produces a result and wants to share that result with another bit of code, what you’re really doing is transferring ownership of that data.
One way to make concurrent programs safe is to ensure only one concurrent context has ownership of data at a time. Channels help us communicate this concept by encoding that intent into the channel’s type.

Are you trying to guard internal state of a struct? Use primitives
This is a great candidate for memory access synchronization primitives, and a pretty strong indicator that you shouldn’t use channels. Remember the key word here is internal. If you find yourself exposing locks beyond a type, this should raise a red flag. Try to keep the locks constrained to a small lexical scope.

Are you trying to coordinate multiple pieces of logic? Use channels
Channels are inherently more composable than memory access synchronization primitives. Having locks scattered throughout your object-graph sounds like a nightmare, but having channels everywhere is expected and encouraged! I can compose channels, but I can’t easily compose locks or methods that return values.

Is it a performance-critical section? Use primitives
It may help, because channels use memory access synchronization to operate, therefore they can only be slower, however, a performance-critical section might be hinting that we need to restructure our program.
