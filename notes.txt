CHAPTER 1 An Introduction to Concurrency

If software was web scale, among other things, you could expect that it would be embarrassingly parallel; that is, web scale software is usually expected to be able to handle hundreds of thousands (or more) of simultaneous workloads by adding more instances of the application.

A race condition occurs when two or more operations must execute in the correct order, but the program has not been written so that this order is guaranteed to be maintained.
Most of the time, this shows up in what’s called a data race, where one concurrent operation attempts to read a variable while at some undetermined time another con‐ current operation is attempting to write to the same variable.

Atomicity
When something is considered atomic, or to have the property of atomicity, this means that within the context that it is operating, it is indivisible, or uninterruptible.


Deadlock
A deadlocked program is one in which all concurrent processes are waiting on one another. In this state, the program will never recover without outside intervention.

The conditions are now known as the Coffman Conditions and are the basis for techniques that help detect, prevent, and correct deadlocks.
The Coffman Conditions are as follows:

Mutual Exclusion
A concurrent process holds exclusive rights to a resource at any one time.

Wait For Condition
A concurrent process must simultaneously hold a resource and be waiting for an additional resource.

No Preemption
A resource held by a concurrent process can only be released by that process, so it fulfills this condition.

Circular Wait
A concurrent process (P1) must be waiting on a chain of other concurrent pro‐ cesses (P2), which are in turn waiting on it (P1), so it fulfills this final condition too.



Livelock
Livelocks are programs that are actively performing concurrent operations, but these operations do nothing to move the state of the program forward.


Starvation
Starvation is any situation where a concurrent process cannot get all the resources it needs to perform work.


 As of Go 1.8, garbage collection pauses are generally between 10 and 100 microseconds!



























=========================================================================================================================


CHAPTER 2 Modeling Your Code: Communicating Sequential Processes


Concurrency is a property of the code; parallelism is a property of the running program.


Before Go if you wanted to write concurrent code, you would model your program in terms of threads and synchronize the access to the memory between them. If you had a lot of things you had to model concurrently and your machine couldn’t handle that many threads, you created a thread pool and multiplexed your operations onto the thread pool.

Go has added another link in that chain: the goroutine. In addition, Go has borrowed several concepts from the work of famed computer scientist Tony Hoare, and introduced new primitives for us to use, namely channels.

We’d assume that introducing another level of abstraction below OS threads would bring with it more difficulties, but the interesting thing is that it doesn’t. It actually makes things easier. This is because we haven’t really added another layer of abstraction on top of OS threads, we’ve supplanted them.

Threads are still there, of course, but we find that we rarely have to think about our problem space in terms of OS threads. Instead, we model things in goroutines and channels, and occasionally shared memory.

CSP stands for “Communicating Sequential Processes,” which is both a technique and the name of the paper that introduced it. In 1978, Charles Antony Richard Hoare published the paper in the Association for Computing Machinery (ACM).

Inputs and outputs needed to be considered language primitives, Hoare’s CSP programming language contained primitives to model input and output, or communication, between processes correctly (this is where the paper’s name comes from). Hoare applied the term processes to any encapsulated portion of logic that required input to run and produced output other processes would consume.

For communication between the processes, Hoare created input and output com‐ mands: ! for sending input into a process, and ? for reading output from a process.

Each command had to specify either an output variable (in the case of reading a variable out of a process), or a destination (in the case of sending input to a process). Sometimes these two would refer to the same thing, in which case the two processes would be said to correspond. In other words, output from one process would flow directly into the input of another process.

Operation 							Explanation

cardreader?cardimage				From cardreader, read a card and assign its value (an array of characters) to the 
									variable cardimage.

lineprinter!line					To lineprinter, send the value of lineimage for printing.

image X?(x, y)						From process named X, input a pair of values and assign them to x and y.

DIV!(3*a+b, 13)						To process DIV, output the two specified values.

*[c:character; west?c → east!c]		Read all the characters output by west, and output them one by one to east. 
									The repetition  terminates when the process west terminates.

The similarities to Go’s channels are apparent. Notice how in the last example the output from west was sent to a variable c and the input to east was received from the same variable. These two processes correspond.

Over the next six years, the idea of CSP was refined into a formal representation of something called process calculus in an effort to take the ideas of communicating sequential processes and actually begin to reason about program correctness. Process calculus is a way to mathematically model concurrent systems and also provides algebraic laws to perform transformations on these systems to analyze their various properties, e.g., efficiency and correctness.

The language also utilized a so-called guarded command, which Edgar Dijkstra had introduced in 1974. (“Guarded commands, nondetermi‐ nacy and formal derivation of programs”).

A guarded command is simply a statement with a left- and righthand side, split by a →.

The lefthand side served as a conditional, or guard for the righthand side in that if the lefthand side was false or, in the case of a command, returned false or had exited, the righthand side would never be executed.

Combining these with Hoare’s I/O commands laid the foundation for Hoare’s communicating processes, and thus Go’s channels.

It’s common for languages to end their chain of abstraction at the level of the OS thread and memory access synchronization. Go takes a different route and supplants this with the concept of goroutines and channels.



Goroutines are lightweight, and we normally won’t have to worry about creating one.

Go’s runtime multiplexes goroutines onto OS threads automatically and manages their scheduling for us.

Channels, for instance, are inherently composable with other channels. This makes writing large systems simpler because you can coordinate the input from multiple subsystems by easily composing the output together. You can combine input channels with timeouts, cancellations, or messages to other subsystems. Coordinating mutexes is a much more difficult proposition.

The select statement is the complement to Go’s channels and is what enables all the difficult bits of composing channels. select statements allow you to efficiently wait for events, select a message from competing channels in a uniform random way, continue on if there are no messages waiting, and more.


Go was designed around CSP; however, Go also supports more traditional means of writing concurrent code through memory access synchronization and the primitives that follow that technique. Structs and methods in the sync and other packages allow you to perform locks, create pools of resources, preempt goroutines, and more.

Package sync provides basic synchronization primitives such as mutual exclusion locks. Other than the Once and WaitGroup types, most are intended for use by low-level library routines. Higher-level synchronization is better done via channels and communication.


Consider structuring your program so that only one goroutine at a time is ever responsible for a particular piece of data. Do not communicate by sharing memory. Instead, share memory by communicating.



Are you trying to transfer ownership of data? Use channels
If you have a bit of code that produces a result and wants to share that result with another bit of code, what you’re really doing is transferring ownership of that data.
One way to make concurrent programs safe is to ensure only one concurrent context has ownership of data at a time. Channels help us communicate this concept by encoding that intent into the channel’s type.

Are you trying to guard internal state of a struct? Use primitives
This is a great candidate for memory access synchronization primitives, and a pretty strong indicator that you shouldn’t use channels. Remember the key word here is internal. If you find yourself exposing locks beyond a type, this should raise a red flag. Try to keep the locks constrained to a small lexical scope.

Are you trying to coordinate multiple pieces of logic? Use channels
Channels are inherently more composable than memory access synchronization primitives. Having locks scattered throughout your object-graph sounds like a nightmare, but having channels everywhere is expected and encouraged! I can compose channels, but I can’t easily compose locks or methods that return values.

Is it a performance-critical section? Use primitives
It may help, because channels use memory access synchronization to operate, therefore they can only be slower, however, a performance-critical section might be hinting that we need to restructure our program.





























=========================================================================================================================

CHAPTER 3 Go’s Concurrency Building Blocks

Goroutines are one of the most basic units of organization in a Go program, in fact, every Go program has at least one goroutine: the main goroutine, which is automatically created and started when the process begins. In almost any program you’ll probably find. They’re not OS threads, and they’re not exactly green threads—threads that are managed by a language’s runtime—they’re a higher level of abstraction known as coroutines.

Coroutines are simply concurrent subroutines (functions, closures, or methods in Go) that are nonpreemptive—that is, they cannot be interrupted. Instead, coroutines have multiple points throughout which allow for suspension or reentry.

Concurrency is not a property of a coroutine(thus goroutine): something must host several coroutines simultaneously and give each an opportunity to execute—otherwise, they wouldn’t be concurrent!

Go’s mechanism for hosting goroutines is an implementation of what’s called an M:N scheduler, which means it maps M green threads to N OS threads.


Go follows a model of concurrency called the fork-join model. The word fork refers to the fact that at any point in the program, it can split off a child branch of execution to be run concurrently with its parent. The word join refers to the fact that at some point in the future, these concurrent branches of execution will join back together. Where the child rejoins the parent is called a join point.


Goroutine is extraordinarily lightweight.
A newly minted goroutine is given a few kilobytes, which is almost always enough. When it isn’t, the run-time grows (and shrinks) the memory for storing the stack auto‐ matically, allowing many goroutines to live in a modest amount of memory. The CPU overhead averages about three cheap instructions per function call. It is practical to create hundreds of thousands of goroutines in the same address space. If goroutines were just threads, system resources would run out at a much smaller number.


Context switching is when something hosting a concurrent process must save its state to switch to running a different concurrent process.

If we have too many concurrent processes, we can spend all of our CPU time context switching between them and never get any real work done.

At the OS level, with threads, this can be quite costly. The OS thread must save things like register values, lookup tables, and memory maps to successfully be able to switch back to the current thread when it is time. Then it has to load the same information for the incoming thread.

Context switching in software is comparatively much, much cheaper. Under a software-defined scheduler, the runtime can be more selective in what is persisted for retrieval, how it is persisted, and when the persisting need occur.


1.467 μs os thread context switching
0.225 μs goroutine context switching, 92% faster than an OS context switch on my machine.


The sync package contains the concurrency primitives that are most useful for low-level memory access synchronization.

Go has built a new set of concurrency primitives on top of the memory access synchronization primitives to provide you with an expanded set of things to work with.

WaitGroup is a great way to wait for a set of concurrent operations to complete when you either don’t care about the result of the concurrent operation, or you have other means of collecting their results. If neither of those conditions are true, I suggest you use channels and a select statement instead.

Mutex stands for “mutual exclusion” and is a way to guard critical sections of your program that requires exclusive access to a shared resource.

Mutex provides a concurrent-safe way to express exclusive access to these shared resources.

Always call Unlock within a defer statement. This is a very common idiom when utilizing a Mutex to ensure the call always happens, even when panicing. Failing to do so will probably cause your program to deadlock.

It is expensive to enter and exit a critical section, and so generally people attempt to minimize the time spent in critical sections.

The sync.RWMutex is conceptually the same thing as a Mutex: it guards access to memory; however, RWMutex gives you a little bit more control over the memory. You can request a lock for reading, in which case you will be granted access unless the lock is being held for writing. This means that an arbitrary number of readers can hold a reader lock so long as nothing else is holding a writer lock.

It’s usually advisable to use RWMutex instead o Mutex when it logically makes sense.


Cond is a rendezvous point for goroutines waiting for or announcing the occurrence of an event.
The NewCond function takes in a type that satisfies the sync.Locker interface.
ex: (sync.Locker(sync.Mutex, sync.RWMutex) has Lock Unlock methods)
c := sync.NewCond(&sync.Mutex{})

c.L.Lock()
for !condition() {
    c.Wait() // Here we wait to be notified that the condition has occurred.
}
//	... make use of condition ...
c.L.Unlock()

// c.Wait documentation says this is common pattern of implementing the code:

//	c.L.Lock()
//	for !condition() {
//	    c.Wait()
//	}
//	... make use of condition ...
//	c.L.Unlock()


condition Wait() doesn’t just block, it suspends the current goroutine, allowing other goroutines to run on the OS thread.

TODO: need to understand this and practice, didn't get it
few other things happen when you call Wait: upon entering Wait, Unlock is called on the Cond variable’s Locker,
and upon exiting Wait, Lock is called on the Cond variable’s Locker.


Signal. This is one of two methods that the Cond type provides for notifying goroutines blocked on a Wait call that the condition has been triggered. The other is a method called Broadcast.

Internally, the runtime maintains a FIFO list of goroutines waiting to be signaled;
Signal finds the goroutine that’s been waiting the longest and notifies that,
whereas Broadcast sends a signal to all goroutines that are waiting.


code(this code is understandable):

package main

import (
	"fmt"
	"sync"
	"time"
)

func main() {
	c := sync.NewCond(&sync.Mutex{})
	queue := make([]interface{}, 0, 10)

	removeFromQueue := func(delay time.Duration) {
		time.Sleep(delay)
		c.L.Lock()
		queue = queue[1:]
		fmt.Println("Removed from queue")
		c.L.Unlock()
		c.Signal()
	}

	for i := 0; i < 10; i++ {
		c.L.Lock()
		for len(queue) == 2 {
			c.Wait()
		}

		fmt.Println("Adding to queue")
		queue = append(queue, struct{}{})
		go removeFromQueue(1 * time.Second)
		c.L.Unlock()
	}
}

/*

Adding to queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Removed from queue
Adding to queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue


*/



sync.Once utilizes some sync primitives internally to ensure that only one call to Do ever calls the function passed in—even on different goroutines.
sync.Once guarantees that your functions are only called once.

Pool is a concurrent-safe implementation of the object pool pattern.
At a high level, a the pool pattern is a way to create and make available a fixed number, or pool, of things for use.


Pool’s primary interface is its Get method.
When called, Get will first check whether there are any available instances within the pool to return to the caller,
and if not, call its New member variable to create a new one.
When finished, callers call Put to place the instance they were working with back in the pool for use by other processes.


code:

package main

import (
	"fmt"
	"sync"
)

func main() {
	var numCalcsCreated int
	calcPool := &sync.Pool{
		New: func() interface{} {
			numCalcsCreated += 1
			mem := make([]byte, 1024)

			// Notice that we are storing the address of the slice of bytes.
			return &mem
		},
	}

	// Seed the pool with 4KB
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	const numWorkers = 1024 * 1024
	var wg sync.WaitGroup
	wg.Add(numWorkers)

	for i := numWorkers; i > 0; i-- {
		go func() {
			defer wg.Done()
			// And here we are asserting the type is a pointer to a slice of bytes.
			mem := calcPool.Get().(*[]byte)

			defer calcPool.Put(mem)
			// Assume something interesting, but quick is being done with
			// this memory.
		}()
	}
	wg.Wait()
	fmt.Printf("%d calculators were created.\n", numCalcsCreated)
}

/*
results may vary

4 calculators were created.

or

6 calculators were created.

or

8calculators were created.

but not 1024*1024
*/


Had I run this example without a sync.Pool, though the results are nondeterministic, in the worst case I could have been attempting to allocate a gigabyte of memory, but as you see from the output, I’ve only allocated 4 KB.


Another common situation where a Pool is useful is for warming a cache of preallocated objects for operations that must run as quickly as possible.

So when working with a Pool, just remember the following points:
 - When instantiating sync.Pool, give it a New member variable that is thread-safe when called.
 - When you receive an instance from Get, make no assumptions regarding the state of the object you receive back.
 - Make sure to call Put when you’re finished with the object you pulled out of the pool.
 Otherwise, the Pool is useless. Usually this is done with defer.
 - Objects in the pool must be roughly uniform in makeup.


Channels are one of the synchronization primitives in Go derived from Hoare’s CSP.
While they can be used to synchronize access of the memory,
they are best used to communicate information between goroutines.

When using channels, you’ll pass a value into a chan variable,
and then somewhere else in your program read it off the channel.

The disparate parts of your program don’t require knowledge of each other,
only a reference to the same place in memory where the channel resides.

This can be done by passing references of channels around your program.


// Here we declare a channel. We say it is “of type” interface{} since the type we’ve declared is the empty interface.
var dataStream chan interface{}

//Here we instantiate the channel using the built-in make function.
dataStream = make(chan interface{})



Channels can also be declared to only support a unidirectional flow of data — that is,
you can define a channel that only supports sending or receiving information.

To declare a unidirectional channel, you’ll simply include the <- operator.

To both declare and instantiate a channel that can only read,
place the <- operator on the lefthand side, like so:

var dataStream <-chan interface{} // only read
dataStream := make(<-chan interface{})


And to declare and create a channel that can only send,
you place the <- operator on the righthand side, like so:

var dataStream chan<- interface{} // only send
dataStream := make(chan<- interface{})


a := <-c // read from channel(NOT a <- c????)

c <- a // write to channel(NOT c<- = a????)
just remember it

a := <-c
c <- a


You don’t often see unidirectional channels instantiated,
but you’ll often see them used as function parameters and return types. 
This is possible because Go will implicitly convert bidirectional channels to unidirectional channels when needed.


package main

import "fmt"

func main() {
	stringStream := make(chan string)
	go func() {
		stringStream <- "Hello channels!"
	}()

	fmt.Println(<-stringStream)
}

// Hello channels!

Just because a goroutine was scheduled,
there was no guarantee that it would run before the process exited;
But channels are said to be blocking.
This means that any goroutine that attempts to write to a channel that is full will wait until the channel has been emptied,
and any goroutine that attempts to read from a channel that is empty will wait until at least one item is placed on it.


even if place timer, it will wait:
package main

import (
	"fmt"
	"time"
)

func main() {
	stringStream := make(chan string)
	go func() {
		time.Sleep(2 * time.Second)
		stringStream <- "Hello channels!"
	}()

	fmt.Println(<-stringStream)
}


Likewise, the anonymous goroutine is attempting to place a string literal on the stringStream, and so the goroutine will not exit until the write is successful.




v, ok := <-c

The second return value is a way for a read operation to indicate whether the read off the channel was a value generated by a write elsewhere in the process, or a default value generated from a closed channel.


To close a channel, we use the close keyword, like so:

c := make(chan interface{})
close(c)

We could continue performing reads on channel indefinitely despite the channel remaining closed.
We cannot write to closed channel,
but you can read from closed channel.


Ranging over a channel.
The range keyword—used in conjunction with the for statement—supports channels as arguments,
and will automatically break the loop when a channel is closed.
This allows for concise iteration over the values on a channel.

code:

package main

import "fmt"

func main() {
	intStream := make(chan int)

	go func() {
		defer close(intStream)

		for i := 1; i <= 5; i++ {
			intStream <- i
		}
	}()

	for integer := range intStream {
		fmt.Printf("%v ", integer)
	}
}

// 1 2 3 4 5

// it will read from even closed channel




Closing a channel is also one of the ways you can signal multiple goroutines simultaneously.
If you have n goroutines waiting on a single channel, instead of writing n times to the channel to unblock each goroutine,
you can simply close the channel.


code:

package main

import (
	"fmt"
	"sync"
)

func main() {
	begin := make(chan interface{})

	var wg sync.WaitGroup
	for i := 0; i < 5; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()

			// Here the goroutine waits until it is told it can continue.
			<-begin

			fmt.Printf("%v has begun\n", i)
		}(i)
	}

	fmt.Println("Unblocking goroutines...")

	// Here we close the channel, thus unblocking all the goroutines simultaneously.
	close(begin)

	wg.Wait()
}

/*

Unblocking goroutines...
0 has begun
3 has begun
1 has begun
2 has begun
4 has begun

*/


Buffered channels are given a capacity when they’re instantiated.

This means that even if no reads are performed on the channel, a goroutine can still perform n writes,
where n is the capacity of the buffered channel.

code:

var dataStream chan interface{}
dataStream = make(chan interface{}, 4)

Here we create a buffered channel with a capacity of four.
This means that we can place four things onto the channel regardless of whether it’s being read from.



Unbuffered channels are also defined in terms of buffered channels:
an unbuffered channel is simply a buffered channel created with a capacity of 0.

a := make(chan int) // unbufferred channel
b := make(chan int, 0) // the same, unbufferred channel
c := make(chan int, 1) // bufferred channel


Writes to a channel block if a channel is full,
and reads from a channel block if the channel is empty.

An unbuffered channel has a capacity of zero and so it’s already full before any writes.
A buffered channel with no receivers and a capacity of four would be full after four writes,
and block on the fifth write since it has nowhere else to place the fifth element.

Like unbuffered channels, buffered channels are still blocking; the preconditions that the channel be empty or full are just different.

In this way, buffered channels are an in-memory FIFO queue for concurrent processes to communicate over.

c := make(chan rune, 4)
[   ][   ][   ][   ]

c<-'a'
['a'][   ][   ][   ]

c<-'b'
['a']['b'][   ][   ]

c<-'c'
['a']['b']['c'][   ]

c<-'d'
['a']['b']['c']['d']

c<-'e'
['a']['b']['c']['d'] 'e'
The goroutine performing this write is blocked!
The goroutine will remain blocked until room is made in the buffer by some goroutine performing a read.

<-c // 'a'
['b']['c']['d']['e']


nil channel
Read from nil channel will panic
Write to nil channel will panic
Close nil channel will panic
Be sure to ensure the channels you’re working with are always initialized.



operation 	Channel state 			Result

Read 		nil 					Block 
			Open and Not Empty 		Value 
			Open and Empty 			Block 
			Closed 					<default value>, false 
			Write Only 				Compilation Error 

Write 		nil 					Block 
			Open and Full 			Block 
			Open and Not Full 		Write Value 
			Closed 					panic 
			Receive Only 			Compilation Error 

close 		nil 					panic 
			Open and Not Empty 		Closes Channel; reads succeed until channel is drained, then reads produce default value Open and empty 		 Closes Channel; reads produces default value
			Closed 					panic
			Receive Only 			Compilation Error





The first thing we should do to put channels in the right context is to assign channel ownership.
I’ll define ownership as being a goroutine that instantiates, writes, and closes a channel.




Let’s begin with channel owners.
The goroutine that owns a channel should:
1. Instantiate the channel.
2. Perform writes, or pass ownership to another goroutine.
3. Close the channel.
4. Ecapsulate the previous three things in this list and expose them via a reader channel.

By assigning these responsibilities to channel owners, a few things happen: 
- Because we’re the one initializing the channel, we remove the risk of deadlocking by writing to a nil channel. 
- Because we’re the one initializing the channel, we remove the risk of panicing by closing a nil channel. 
- Because we’re the one who decides when the channel gets closed, we remove the risk of panicing by writing to a closed channel. 
- Because we’re the one who decides when the channel gets closed, we remove the risk of panicing by closing a channel more than once. 
- We wield the type checker at compile time to prevent improper writes to our channel.



As a consumer of a channel, I only have to worry about two things:
- Knowing when a channel is closed.
- Responsibly handling blocking for any reason.




Channels are the glue that binds goroutines together.
The select statement is the glue that binds channels together;
it’s how we’re able to compose channels together in a program to form larger abstractions.



var c1, c2 <-chan interface{}
var c3 chan<- interface{}
select {
case <- c1:
	// Do something
case <- c2:
	// Do something
case c3<- struct{}{}: 
	// Do something 
}

Unlike switch blocks, case statements in a select block aren’t tested sequentially, and execution won’t automatically fall through if none of the criteria are met.


If none of the channels are ready, the entire select statement blocks.
Then when one the channels is ready, that operation will proceed, and its corresponding statements will execute.


code:
package main

import (
	"fmt"
	"time"
)

func main() {
	start := time.Now()

	c := make(chan interface{})
	go func() {
		time.Sleep(5 * time.Second)

		// Here we close the channel after waiting five seconds.
		close(c)
	}()

	fmt.Println("Blocking on read...")
	select {
	// Here we attempt a read on the channel.
	case <-c:
		fmt.Printf("Unblocked %v later.\n", time.Since(start))
	}
}

// We only unblock roughly five seconds after entering the select block.
// This is a simple and efficient way to block while we’re waiting for something to happen.

/*

Blocking on read...
Unblocked 5.003761973s later.

*/



code:

package main

import "fmt"

func main() {
	c1 := make(chan interface{})
	close(c1)
	c2 := make(chan interface{})
	close(c2)

	var c1Count, c2Count int
	for i := 1000; i >= 0; i-- {
		select {
		case <-c1:
			c1Count++
		case <-c2:
			c2Count++
		}
	}

	fmt.Printf("c1Count: %d\nc2Count: %d\n", c1Count, c2Count)
}

/*

c1Count: 496
c2Count: 505

*/


In a thousand iterations, roughly half the time the select statement read from c1,
and roughly half the time it read from c2. That seems interesting, and maybe a bit too coincidental.
In fact, it is! The Go runtime will perform a pseudorandom uniform selection over the set of case statements.
This just means that of your set of case statements, each has an equal chance of being selected as all the others.



If there’s nothing useful you can do when all the channels are blocked, but you also can’t block forever, you may want to time out.

code:

package main

import (
	"fmt"
	"time"
)

func main() {
	var c <-chan int
	select {
	case <-c:
		// do something
	case <-time.After(1 * time.Second):
		fmt.Println("Timed out.")
	}
}


/*

Timed out.

*/





What happens when no channel is ready, and we need to do something in the meantime?
The select statement also allows for a default clause in case you’d like to do something if all the channels you’re selecting against are blocking.

code:

package main

import (
	"fmt"
	"time"
)

func main() {

	start := time.Now()
	var c1, c2 <-chan int
	select {
	case <-c1:
		// do something
	case <-c2:
		// do something
	default:
		fmt.Printf("In default after %v\n\n", time.Since(start))
	}
}

// You can see that it ran the default statement almost instantaneously.
// This allows you to exit a select block without blocking.

/*

In default after 2.76µs

*/



Usually you’ll see a default clause used in conjunction with a for-select loop.
This allows a goroutine to make progress on work while waiting for another goroutine to report a result.

code:

package main

import (
	"fmt"
	"time"
)

func main() {
	done := make(chan interface{})

	go func() {
		time.Sleep(5 * time.Second)

		close(done)
	}()

	workCounter := 0

loop:
	for {
		select {
		case <-done:
			break loop
		default:
		}

		// Simulate work
		workCounter++
		time.Sleep(1 * time.Second)
	}

	fmt.Printf("Achieved %v cycles of work before signalled to stop.\n", workCounter)
}

// Achieved 5 cycles of work before signalled to stop.



GOMAXPROCS. the name is misleading: people often think this function relates to the number of logical processors on the host machine—and in a roundabout way it does—but really this function controls the number of OS threads that will host so-called “work queues.”





























=========================================================================================================================

CHAPTER 4 Concurrency Patterns in Go

When working with concurrent code, there are a few different options for safe operation:
- Synchronization primitives for sharing memory (e.g., sync.Mutex)
- Synchronization via communicating (e.g., channels)

However, there are a couple of other options that are implicitly safe within multiple concurrent processes:
- Immutable data
- Data protected by confinement



Immutable data is implicitly concurrent-safe.
Each concurrent process may operate on the same data, but it may not modify it.
If it wants to create new data, it must create a new copy of the data with the desired modifications.

Confinement is ensuring information is only ever available from one concurrent process.
There are two kinds of confinement possible: ad hoc and lexical.

Ad hoc confinement is when you achieve confinement through a convention. ???


Because of the lexical scope, we’ve made it impossible1 to do the wrong thing,
and so we don’t need to synchronize memory access or share data through communication.

Improved performance and reduced cognitive load on developers.
Synchronization comes with a cost, and if you can avoid it you won’t have any critical sections,
and therefore you won’t have to pay the cost of synchronizing them.


for-select loop.

// Either loop infinitely or range over something
for {
	// Do some work with channels 
	select {
	}
}


Sending iteration variables out on a channel.
for _, s := range []string{"a", "b", "c"} {
	select {
		case <-done:
			return
		case stringStream <- s:
	}
}


Looping infinitely waiting to be stopped.
for {
	select {
		case <-done:
			return 
		default: 
	} 

	// Do non-preemptable work 
}



Goroutines are cheap and easy to create, but do cost resources, and are not garbage collected by the runtime.

The goroutine has a few paths to termination:
 - When it has completed its work.
 - When it cannot continue its work due to an unrecoverable error.
 - When it’s told to stop working.



code:
package main

import (
	"fmt"
	"time"
)

func main() {

	// Here we pass the done channel to the doWork function.
	// As a convention, this channel is the first parameter.
	doWork := func(
		done <-chan interface{},
		strings <-chan string,
	) <-chan interface{} {
		terminated := make(chan interface{})

		go func() {
			defer fmt.Println("doWork exited.")
			defer close(terminated)

			for {
				select {
				case s := <-strings:
					fmt.Println("a") // not printed
					// Do something interesting
					fmt.Println(s)

				// On this line we see the ubiquitous for-select pattern in use.
				// One of our case statements is checking whether our done channel
				// has been signaled. If it has, we return from the goroutine.
				case <-done:
					return // if we comment this line then we will stuck // Canceling doWork goroutine...
				}
			}
		}()

		return terminated
	}

	done := make(chan interface{})

	terminated := doWork(done, nil)

	// Here we create another goroutine that will cancel the goroutine spawned in doWork
	// if more than one second passes.
	go func() {
		// Cancel the operation after 1 second.
		time.Sleep(1 * time.Second)
		fmt.Println("Canceling doWork goroutine...")
		close(done)
	}()

	// This is where we join the goroutine spawned from doWork with the main goroutine.
	<-terminated
	fmt.Println("Done.")
}

/*

Canceling doWork goroutine...
doWork exited.
Done.

*/


PROGRESS ==> PAGE 93

