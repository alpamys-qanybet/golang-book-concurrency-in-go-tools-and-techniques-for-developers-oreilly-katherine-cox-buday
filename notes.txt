CHAPTER 1 An Introduction to Concurrency

If software was web scale, among other things, you could expect that it would be embarrassingly parallel; that is, web scale software is usually expected to be able to handle hundreds of thousands (or more) of simultaneous workloads by adding more instances of the application.

A race condition occurs when two or more operations must execute in the correct order, but the program has not been written so that this order is guaranteed to be maintained.
Most of the time, this shows up in what’s called a data race, where one concurrent operation attempts to read a variable while at some undetermined time another con‐ current operation is attempting to write to the same variable.

Atomicity
When something is considered atomic, or to have the property of atomicity, this means that within the context that it is operating, it is indivisible, or uninterruptible.


Deadlock
A deadlocked program is one in which all concurrent processes are waiting on one another. In this state, the program will never recover without outside intervention.

The conditions are now known as the Coffman Conditions and are the basis for techniques that help detect, prevent, and correct deadlocks.
The Coffman Conditions are as follows:

Mutual Exclusion
A concurrent process holds exclusive rights to a resource at any one time.

Wait For Condition
A concurrent process must simultaneously hold a resource and be waiting for an additional resource.

No Preemption
A resource held by a concurrent process can only be released by that process, so it fulfills this condition.

Circular Wait
A concurrent process (P1) must be waiting on a chain of other concurrent pro‐ cesses (P2), which are in turn waiting on it (P1), so it fulfills this final condition too.



Livelock
Livelocks are programs that are actively performing concurrent operations, but these operations do nothing to move the state of the program forward.


Starvation
Starvation is any situation where a concurrent process cannot get all the resources it needs to perform work.


 As of Go 1.8, garbage collection pauses are generally between 10 and 100 microseconds!


=========================================================================================================================


CHAPTER 2 Modeling Your Code: Communicating Sequential Processes


Concurrency is a property of the code; parallelism is a property of the running program.


Before Go if you wanted to write concurrent code, you would model your program in terms of threads and synchronize the access to the memory between them. If you had a lot of things you had to model concurrently and your machine couldn’t handle that many threads, you created a thread pool and multiplexed your operations onto the thread pool.

Go has added another link in that chain: the goroutine. In addition, Go has borrowed several concepts from the work of famed computer scientist Tony Hoare, and introduced new primitives for us to use, namely channels.

We’d assume that introducing another level of abstraction below OS threads would bring with it more difficulties, but the interesting thing is that it doesn’t. It actually makes things easier. This is because we haven’t really added another layer of abstraction on top of OS threads, we’ve supplanted them.

Threads are still there, of course, but we find that we rarely have to think about our problem space in terms of OS threads. Instead, we model things in goroutines and channels, and occasionally shared memory.

CSP stands for “Communicating Sequential Processes,” which is both a technique and the name of the paper that introduced it. In 1978, Charles Antony Richard Hoare published the paper in the Association for Computing Machinery (ACM).

Inputs and outputs needed to be considered language primitives, Hoare’s CSP programming language contained primitives to model input and output, or communication, between processes correctly (this is where the paper’s name comes from). Hoare applied the term processes to any encapsulated portion of logic that required input to run and produced output other processes would consume.

For communication between the processes, Hoare created input and output com‐ mands: ! for sending input into a process, and ? for reading output from a process.

Each command had to specify either an output variable (in the case of reading a variable out of a process), or a destination (in the case of sending input to a process). Sometimes these two would refer to the same thing, in which case the two processes would be said to correspond. In other words, output from one process would flow directly into the input of another process.

Operation 							Explanation

cardreader?cardimage				From cardreader, read a card and assign its value (an array of characters) to the 
									variable cardimage.

lineprinter!line					To lineprinter, send the value of lineimage for printing.

image X?(x, y)						From process named X, input a pair of values and assign them to x and y.

DIV!(3*a+b, 13)						To process DIV, output the two specified values.

*[c:character; west?c → east!c]		Read all the characters output by west, and output them one by one to east. 
									The repetition  terminates when the process west terminates.

The similarities to Go’s channels are apparent. Notice how in the last example the output from west was sent to a variable c and the input to east was received from the same variable. These two processes correspond.

Over the next six years, the idea of CSP was refined into a formal representation of something called process calculus in an effort to take the ideas of communicating sequential processes and actually begin to reason about program correctness. Process calculus is a way to mathematically model concurrent systems and also provides algebraic laws to perform transformations on these systems to analyze their various properties, e.g., efficiency and correctness.

The language also utilized a so-called guarded command, which Edgar Dijkstra had introduced in 1974. (“Guarded commands, nondetermi‐ nacy and formal derivation of programs”).

A guarded command is simply a statement with a left- and righthand side, split by a →.

The lefthand side served as a conditional, or guard for the righthand side in that if the lefthand side was false or, in the case of a command, returned false or had exited, the righthand side would never be executed.

Combining these with Hoare’s I/O commands laid the foundation for Hoare’s communicating processes, and thus Go’s channels.

It’s common for languages to end their chain of abstraction at the level of the OS thread and memory access synchronization. Go takes a different route and supplants this with the concept of goroutines and channels.



Goroutines are lightweight, and we normally won’t have to worry about creating one.

Go’s runtime multiplexes goroutines onto OS threads automatically and manages their scheduling for us.

Channels, for instance, are inherently composable with other channels. This makes writing large systems simpler because you can coordinate the input from multiple subsystems by easily composing the output together. You can combine input channels with timeouts, cancellations, or messages to other subsystems. Coordinating mutexes is a much more difficult proposition.

The select statement is the complement to Go’s channels and is what enables all the difficult bits of composing channels. select statements allow you to efficiently wait for events, select a message from competing channels in a uniform random way, continue on if there are no messages waiting, and more.


Go was designed around CSP; however, Go also supports more traditional means of writing concurrent code through memory access synchronization and the primitives that follow that technique. Structs and methods in the sync and other packages allow you to perform locks, create pools of resources, preempt goroutines, and more.

Package sync provides basic synchronization primitives such as mutual exclusion locks. Other than the Once and WaitGroup types, most are intended for use by low-level library routines. Higher-level synchronization is better done via channels and communication.


Consider structuring your program so that only one goroutine at a time is ever responsible for a particular piece of data. Do not communicate by sharing memory. Instead, share memory by communicating.



Are you trying to transfer ownership of data? Use channels
If you have a bit of code that produces a result and wants to share that result with another bit of code, what you’re really doing is transferring ownership of that data.
One way to make concurrent programs safe is to ensure only one concurrent context has ownership of data at a time. Channels help us communicate this concept by encoding that intent into the channel’s type.

Are you trying to guard internal state of a struct? Use primitives
This is a great candidate for memory access synchronization primitives, and a pretty strong indicator that you shouldn’t use channels. Remember the key word here is internal. If you find yourself exposing locks beyond a type, this should raise a red flag. Try to keep the locks constrained to a small lexical scope.

Are you trying to coordinate multiple pieces of logic? Use channels
Channels are inherently more composable than memory access synchronization primitives. Having locks scattered throughout your object-graph sounds like a nightmare, but having channels everywhere is expected and encouraged! I can compose channels, but I can’t easily compose locks or methods that return values.

Is it a performance-critical section? Use primitives
It may help, because channels use memory access synchronization to operate, therefore they can only be slower, however, a performance-critical section might be hinting that we need to restructure our program.


=========================================================================================================================

CHAPTER 3 Go’s Concurrency Building Blocks

Goroutines are one of the most basic units of organization in a Go program, in fact, every Go program has at least one goroutine: the main goroutine, which is automatically created and started when the process begins. In almost any program you’ll probably find. They’re not OS threads, and they’re not exactly green threads—threads that are managed by a language’s runtime—they’re a higher level of abstraction known as coroutines.

Coroutines are simply concurrent subroutines (functions, closures, or methods in Go) that are nonpreemptive—that is, they cannot be interrupted. Instead, coroutines have multiple points throughout which allow for suspension or reentry.

Concurrency is not a property of a coroutine(thus goroutine): something must host several coroutines simultaneously and give each an opportunity to execute—otherwise, they wouldn’t be concurrent!

Go’s mechanism for hosting goroutines is an implementation of what’s called an M:N scheduler, which means it maps M green threads to N OS threads.


Go follows a model of concurrency called the fork-join model. The word fork refers to the fact that at any point in the program, it can split off a child branch of execution to be run concurrently with its parent. The word join refers to the fact that at some point in the future, these concurrent branches of execution will join back together. Where the child rejoins the parent is called a join point.


Goroutine is extraordinarily lightweight.
A newly minted goroutine is given a few kilobytes, which is almost always enough. When it isn’t, the run-time grows (and shrinks) the memory for storing the stack auto‐ matically, allowing many goroutines to live in a modest amount of memory. The CPU overhead averages about three cheap instructions per function call. It is practical to create hundreds of thousands of goroutines in the same address space. If goroutines were just threads, system resources would run out at a much smaller number.


Context switching is when something hosting a concurrent process must save its state to switch to running a different concurrent process.

If we have too many concurrent processes, we can spend all of our CPU time context switching between them and never get any real work done.

At the OS level, with threads, this can be quite costly. The OS thread must save things like register values, lookup tables, and memory maps to successfully be able to switch back to the current thread when it is time. Then it has to load the same information for the incoming thread.

Context switching in software is comparatively much, much cheaper. Under a software-defined scheduler, the runtime can be more selective in what is persisted for retrieval, how it is persisted, and when the persisting need occur.


1.467 μs os thread context switching
0.225 μs goroutine context switching, 92% faster than an OS context switch on my machine.


The sync package contains the concurrency primitives that are most useful for low-level memory access synchronization.

Go has built a new set of concurrency primitives on top of the memory access synchronization primitives to provide you with an expanded set of things to work with.

WaitGroup is a great way to wait for a set of concurrent operations to complete when you either don’t care about the result of the concurrent operation, or you have other means of collecting their results. If neither of those conditions are true, I suggest you use channels and a select statement instead.

Mutex stands for “mutual exclusion” and is a way to guard critical sections of your program that requires exclusive access to a shared resource.

Mutex provides a concurrent-safe way to express exclusive access to these shared resources.

Always call Unlock within a defer statement. This is a very common idiom when utilizing a Mutex to ensure the call always happens, even when panicing. Failing to do so will probably cause your program to deadlock.

It is expensive to enter and exit a critical section, and so generally people attempt to minimize the time spent in critical sections.

The sync.RWMutex is conceptually the same thing as a Mutex: it guards access to memory; however, RWMutex gives you a little bit more control over the memory. You can request a lock for reading, in which case you will be granted access unless the lock is being held for writing. This means that an arbitrary number of readers can hold a reader lock so long as nothing else is holding a writer lock.

It’s usually advisable to use RWMutex instead o Mutex when it logically makes sense.


Cond is a rendezvous point for goroutines waiting for or announcing the occurrence of an event.
The NewCond function takes in a type that satisfies the sync.Locker interface.
ex: (sync.Locker(sync.Mutex, sync.RWMutex) has Lock Unlock methods)
c := sync.NewCond(&sync.Mutex{})

c.L.Lock()
for !condition() {
    c.Wait() // Here we wait to be notified that the condition has occurred.
}
//	... make use of condition ...
c.L.Unlock()

// c.Wait documentation says this is common pattern of implementing the code:

//	c.L.Lock()
//	for !condition() {
//	    c.Wait()
//	}
//	... make use of condition ...
//	c.L.Unlock()


condition Wait() doesn’t just block, it suspends the current goroutine, allowing other goroutines to run on the OS thread.

TODO: need to understand this and practice, didn't get it
few other things happen when you call Wait: upon entering Wait, Unlock is called on the Cond variable’s Locker,
and upon exiting Wait, Lock is called on the Cond variable’s Locker.


Signal. This is one of two methods that the Cond type provides for notifying goroutines blocked on a Wait call that the condition has been triggered. The other is a method called Broadcast.

Internally, the runtime maintains a FIFO list of goroutines waiting to be signaled;
Signal finds the goroutine that’s been waiting the longest and notifies that,
whereas Broadcast sends a signal to all goroutines that are waiting.


code(this code is understandable):

package main

import (
	"fmt"
	"sync"
	"time"
)

func main() {
	c := sync.NewCond(&sync.Mutex{})
	queue := make([]interface{}, 0, 10)

	removeFromQueue := func(delay time.Duration) {
		time.Sleep(delay)
		c.L.Lock()
		queue = queue[1:]
		fmt.Println("Removed from queue")
		c.L.Unlock()
		c.Signal()
	}

	for i := 0; i < 10; i++ {
		c.L.Lock()
		for len(queue) == 2 {
			c.Wait()
		}

		fmt.Println("Adding to queue")
		queue = append(queue, struct{}{})
		go removeFromQueue(1 * time.Second)
		c.L.Unlock()
	}
}

/*

Adding to queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Removed from queue
Adding to queue
Adding to queue
Removed from queue
Adding to queue
Removed from queue
Adding to queue


*/



sync.Once utilizes some sync primitives internally to ensure that only one call to Do ever calls the function passed in—even on different goroutines.
sync.Once guarantees that your functions are only called once.

Pool is a concurrent-safe implementation of the object pool pattern.
At a high level, a the pool pattern is a way to create and make available a fixed number, or pool, of things for use.


Pool’s primary interface is its Get method.
When called, Get will first check whether there are any available instances within the pool to return to the caller,
and if not, call its New member variable to create a new one.
When finished, callers call Put to place the instance they were working with back in the pool for use by other processes.


code:

package main

import (
	"fmt"
	"sync"
)

func main() {
	var numCalcsCreated int
	calcPool := &sync.Pool{
		New: func() interface{} {
			numCalcsCreated += 1
			mem := make([]byte, 1024)

			// Notice that we are storing the address of the slice of bytes.
			return &mem
		},
	}

	// Seed the pool with 4KB
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	calcPool.Put(calcPool.New())
	const numWorkers = 1024 * 1024
	var wg sync.WaitGroup
	wg.Add(numWorkers)

	for i := numWorkers; i > 0; i-- {
		go func() {
			defer wg.Done()
			// And here we are asserting the type is a pointer to a slice of bytes.
			mem := calcPool.Get().(*[]byte)

			defer calcPool.Put(mem)
			// Assume something interesting, but quick is being done with
			// this memory.
		}()
	}
	wg.Wait()
	fmt.Printf("%d calculators were created.\n", numCalcsCreated)
}

/*
results may vary

4 calculators were created.

or

6 calculators were created.

or

8calculators were created.

but not 1024*1024
*/


Had I run this example without a sync.Pool, though the results are nondeterministic, in the worst case I could have been attempting to allocate a gigabyte of memory, but as you see from the output, I’ve only allocated 4 KB.


Another common situation where a Pool is useful is for warming a cache of preallocated objects for operations that must run as quickly as possible.

So when working with a Pool, just remember the following points:
 - When instantiating sync.Pool, give it a New member variable that is thread-safe when called.
 - When you receive an instance from Get, make no assumptions regarding the state of the object you receive back.
 - Make sure to call Put when you’re finished with the object you pulled out of the pool.
 Otherwise, the Pool is useless. Usually this is done with defer.
 - Objects in the pool must be roughly uniform in makeup.


Channels are one of the synchronization primitives in Go derived from Hoare’s CSP.
While they can be used to synchronize access of the memory,
they are best used to communicate information between goroutines.

When using channels, you’ll pass a value into a chan variable,
and then somewhere else in your program read it off the channel.

The disparate parts of your program don’t require knowledge of each other,
only a reference to the same place in memory where the channel resides.

This can be done by passing references of channels around your program.


// Here we declare a channel. We say it is “of type” interface{} since the type we’ve declared is the empty interface.
var dataStream chan interface{}

//Here we instantiate the channel using the built-in make function.
dataStream = make(chan interface{})



Channels can also be declared to only support a unidirectional flow of data — that is,
you can define a channel that only supports sending or receiving information.

To declare a unidirectional channel, you’ll simply include the <- operator.

To both declare and instantiate a channel that can only read,
place the <- operator on the lefthand side, like so:

var dataStream <-chan interface{} // only read
dataStream := make(<-chan interface{})


And to declare and create a channel that can only send,
you place the <- operator on the righthand side, like so:

var dataStream chan<- interface{} // only send
dataStream := make(chan<- interface{})


a := <-c // read from channel(NOT a <- c????)

c <- a // write to channel(NOT c<- = a????)
just remember it

a := <-c
c <- a


You don’t often see unidirectional channels instantiated,
but you’ll often see them used as function parameters and return types. 
This is possible because Go will implicitly convert bidirectional channels to unidirectional channels when needed.


package main

import "fmt"

func main() {
	stringStream := make(chan string)
	go func() {
		stringStream <- "Hello channels!"
	}()

	fmt.Println(<-stringStream)
}

// Hello channels!

Just because a goroutine was scheduled,
there was no guarantee that it would run before the process exited;
But channels are said to be blocking.
This means that any goroutine that attempts to write to a channel that is full will wait until the channel has been emptied,
and any goroutine that attempts to read from a channel that is empty will wait until at least one item is placed on it.


even if place timer, it will wait:
package main

import (
	"fmt"
	"time"
)

func main() {
	stringStream := make(chan string)
	go func() {
		time.Sleep(2 * time.Second)
		stringStream <- "Hello channels!"
	}()

	fmt.Println(<-stringStream)
}


Likewise, the anonymous goroutine is attempting to place a string literal on the stringStream, and so the goroutine will not exit until the write is successful.

PROGRESS ==> PAGE 68
